#### Awesome Repositories
- [Diffusion Models and Representation Learning: A Survey](https://github.com/dongzhuoyao/Diffusion-Representation-Learning-Survey-Taxonomy)


#### Efficacy

- [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)
  - Code: https://github.com/hojonathanho/diffusion
  - Mini Code: https://github.com/tqch/ddpm-torch
  - Mini Code: https://github.com/w86763777/pytorch-ddpm

- [Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/abs/2011.13456)
  - Code: https://github.com/yang-song/score_sde

- [Generative Modeling by Estimating Gradients of the Data Distribution](https://arxiv.org/abs/1907.05600)
  - Code: https://github.com/ermongroup/ncsn

- [Elucidating the Design Space of Diffusion-Based Generative Models](https://arxiv.org/abs/2206.00364)
  - Code: https://github.com/nvlabs/edm
  - Mini Code: https://github.com/yuanzhi-zhu/mini_edm/tree/main

- [Analyzing and Improving the Training Dynamics of Diffusion Models](https://arxiv.org/abs/2312.02696)
  - Code: https://github.com/nvlabs/edm2
  - Mini Code: https://github.com/mmathew23/improved_edm
  - Mini Code: https://github.com/FutureXiang/edm2
  - Mini Code: https://github.com/YichengDWu/tinyedm


#### Sampling Efficiency

- [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502)
  - Code: https://github.com/ermongroup/ddim
  - Mini Code: https://github.com/Alokia/diffusion-DDIM-pytorch

- [Consistency Models](https://arxiv.org/abs/2303.01469)
  - Code: https://github.com/openai/consistency_models
  - Mini Code: https://github.com/openai/consistency_models_cifar10
  - Mini Code: https://github.com/Kinyugo/consistency_models
  - Mini Code: https://github.com/junhsss/consistency-models

- [Improved Techniques for Training Consistency Models](https://arxiv.org/abs/2310.14189)
  - Code: https://github.com/Kinyugo/consistency_models

- [Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models](https://arxiv.org/abs/2410.11081)

- [Consistency Models Made Easy](https://arxiv.org/abs/2406.14548)
  - Code: https://github.com/locuslab/ect

- [Stable Consistency Tuning: Understanding and Improving Consistency Models](https://arxiv.org/abs/2410.18958)
  - Code: https://github.com/G-U-N/Stable-Consistency-Tuning

- [Scaling Rectified Flow Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2403.03206)

- [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
  - Code: https://github.com/CompVis/latent-diffusion

- [Phased Consistency Model](https://arxiv.org/abs/2405.18407)
 - Code: https://github.com/G-U-N/Phased-Consistency-Model


#### Training Efficiency

- [Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think](https://arxiv.org/abs/2410.06940)
  - Code: https://github.com/sihyun-yu/REPA


- [Return of Unconditional Generation: A Self-supervised Representation Generation Method](https://arxiv.org/abs/2312.03701)
  - Code: https://github.com/LTH14/rcg

#### Interesting Applications

- [Diffusion Model as Representation Learner](https://arxiv.org/abs/2308.10916)
  - Code: https://github.com/Adamdad/Repfusion
  - Interesting Point: Our study begins by examining the feature space of DPMs, revealing that DPMs are inherently denoising autoencoders that balance the representation learning with regularizing model capacity.

- [SODA: Bottleneck Diffusion Models for Representation Learning](https://arxiv.org/abs/2311.17901)
  - Interesting Point: What I cannot create, I do not understand.

- [Self-Improving Diffusion Models with Synthetic Data](https://arxiv.org/abs/2408.16333v1)
  - Interesting Point: Self-IMproving diffusion models with Synthetic data (SIMS) is a new training concept for diffusion models that uses self-synthesized data to provide negative guidance during the generation process to steer a model's generative process away from the non-ideal synthetic data manifold and towards the real data distribution.

- [In-Context LoRA for Diffusion Transformers](https://arxiv.org/abs/2410.23775v3)
  - Code: https://github.com/ali-vilab/In-Context-LoRA


- [Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model](https://arxiv.org/abs/2212.00490v2)
  - Code: https://github.com/wyhuai/ddnm

- [Scaling Rectified Flow Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2403.03206)


#### Theoritical Analysis

- [Generalization in diffusion models arises from geometry-adaptive harmonic representations](https://arxiv.org/abs/2310.02557)
  - Code: https://github.com/LabForComputationalVision/memorization_generalization_in_diffusion_models
  - Interesting Point: Finally, we show that when trained on regular image classes for which the optimal basis is known to be geometry-adaptive and harmonic, the denoising performance of the networks is near-optimal.